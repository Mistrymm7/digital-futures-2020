{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demo_Style_Transfer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"J086V2UEAkQU","colab_type":"text"},"source":["# Fast Style Transfer\n","\n","[Repository](https://github.com/NVIDIA/FastPhotoStyle)\n","\n","Make sure to allocate a GPU to your runtime:\n","Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU"]},{"cell_type":"markdown","metadata":{"id":"JCAqWXKFvCQW","colab_type":"text"},"source":["## Setting Up Environment"]},{"cell_type":"code","metadata":{"id":"C4hBBM-k392V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593602254926,"user_tz":-120,"elapsed":5616,"user":{"displayName":"Ekaterina Vititneva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOPwsdev6jvmyMl-oAuMGDpOuDM0wRLTpeeRIKFQ=s64","userId":"05134176801136046196"}},"outputId":"fdb2b4bd-cbb4-470a-b68d-fde17e9ba33b"},"source":["!git clone --recursive https://github.com/NVIDIA/FastPhotoStyle; \\\n","    rm -dr sample_data; \\\n","    mv FastPhotoStyle/* .; \\\n","    rm -drf FastPhotoStyle; \\\n","    mkdir images; \\\n","    mkdir results"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'FastPhotoStyle'...\n","remote: Enumerating objects: 337, done.\u001b[K\n","remote: Total 337 (delta 0), reused 0 (delta 0), pack-reused 337\u001b[K\n","Receiving objects: 100% (337/337), 50.73 MiB | 28.64 MiB/s, done.\n","Resolving deltas: 100% (194/194), done.\n","rm: cannot remove 'sample_data': No such file or directory\n","mv: cannot move 'FastPhotoStyle/PhotoWCTModels' to './PhotoWCTModels': Directory not empty\n","mkdir: cannot create directory ‘images’: File exists\n","mkdir: cannot create directory ‘results’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MjQJxtZK89Kv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1593602272560,"user_tz":-120,"elapsed":11565,"user":{"displayName":"Ekaterina Vititneva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOPwsdev6jvmyMl-oAuMGDpOuDM0wRLTpeeRIKFQ=s64","userId":"05134176801136046196"}},"outputId":"d487478a-b685-4dc9-e131-887f3a330be3"},"source":["!apt-get install -y axel imagemagick\n","!pip uninstall -y scipy\n","!pip install pynvrtc scipy==1.2.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","axel is already the newest version (2.16.1-1build1).\n","imagemagick is already the newest version (8:6.9.7.4+dfsg-16ubuntu6.8).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n","Uninstalling scipy-1.2.2:\n","  Successfully uninstalled scipy-1.2.2\n","Requirement already satisfied: pynvrtc in /usr/local/lib/python3.6/dist-packages (9.2)\n","Collecting scipy==1.2.2\n","  Using cached https://files.pythonhosted.org/packages/83/69/20c8f3b7efe362093dff891239551ff90d4c463b5f52676e2694fea09442/scipy-1.2.2-cp36-cp36m-manylinux1_x86_64.whl\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.2) (1.18.5)\n","\u001b[31mERROR: umap-learn 0.4.4 has requirement scipy>=1.3.1, but you'll have scipy 1.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy\n","Successfully installed scipy-1.2.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1TbuTHanvE03","colab_type":"text"},"source":["## Preprocessing Input Images and Making Inferences"]},{"cell_type":"code","metadata":{"id":"HdKhN92W4bAH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1593602135043,"user_tz":-120,"elapsed":10591,"user":{"displayName":"Ekaterina Vititneva","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOPwsdev6jvmyMl-oAuMGDpOuDM0wRLTpeeRIKFQ=s64","userId":"05134176801136046196"}},"outputId":"685a669b-1840-4505-f43d-711efd027c6c"},"source":["!cd images; \\\n","    content_img_url=\"http://freebigpictures.com/wp-content/uploads/shady-forest.jpg\"; \\\n","    style_img_url=\"https://vignette.wikia.nocookie.net/strangerthings8338/images/e/e0/Wiki-background.jpeg/revision/latest?cb=20170522192233\"; \\\n","    axel -q -n 1 ${content_img_url} --output=content1.png; \\\n","    axel -q -n 1 ${style_img_url} --output=style1.png\n","\n","!convert -resize 25% images/content1.png images/content1.png\n","!convert -resize 50% images/style1.png images/style1.png\n","\n","!dt=$(date +\"%Y%m%d%H%M%S\"); \\\n","    python demo.py --output_image_path results/transfered-${dt}.png"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No state file, cannot resume!\n","No state file, cannot resume!\n","Resize image: (13,9)->(13,9)\n","Resize image: (120,68)->(120,68)\n","Elapsed time in stylization: 0.036623\n","Traceback (most recent call last):\n","  File \"demo.py\", line 47, in <module>\n","    no_post=args.no_post\n","  File \"/content/process_stylization.py\", line 122, in stylization\n","    stylized_img = stylization_module.transform(cont_img, styl_img, cont_seg, styl_seg)\n","  File \"/content/photo_wct.py\", line 30, in transform\n","    cF4, cpool_idx, cpool1, cpool_idx2, cpool2, cpool_idx3, cpool3 = self.e4(cont_img)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/models.py\", line 125, in forward\n","    out = self.pad4_1(out)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/padding.py\", line 163, in forward\n","    return F.pad(input, self.padding, 'reflect')\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 3408, in _pad\n","    return torch._C._nn.reflection_pad2d(input, pad)\n","RuntimeError: Padding size should be less than the corresponding input dimension, but got: padding (1, 1) at dimension 3 of input [1, 256, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y2jgxM0kyqOY","colab_type":"text"},"source":["Developed by the City Intelligence Lab, Austrian Institute of Technology GmbH"]}]}